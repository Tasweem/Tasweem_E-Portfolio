<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	This is an example of a sub page for each module.  It has to be replicated in each module, containing the requested contents -  artefacts, notes, reflections etc
	Ensure you give a different title to each replica and link it to the main module page accordingly.
-->
<html>
	<head>
		<title>Tasweem Beelunkhan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="css/main.css" />
		<noscript><link rel="stylesheet" href="css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><span>Tasweem Beelunkhan</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="About.html">About Me</a></li>
							<li><a href="Launch Module.html">The Data Professional</a></li>
							<li><a href="Module 2.html">Numerical Analysis</a></li>
							<li><a href="Module 4.html">Deciphering Big Data</a></li>
							<li><a href="Module 3.html">Visualising Data</a></li>
							<li><a href="Module 5.html">Machine Learning</a></li>
							<li><a href="Module 6.html">Research Methods and Professional Practice</a></li>
							<li><a href="Project.html">Project</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1> &#10021; Unit 5: Clustering </h1>
										<h3>Outcomes from the Team Exercises and activites</h3>
									</header>

                                                                 						

										    <p>In data analysis and machine learning, clustering—more specifically, K-means clustering—is a well-known technique for organising data points according to their commonalities. The idea of similarity and dissimilarity between data objects is at the heart of clustering, and it is assessed using a variety of metrics, including Jaccard, Manhattan, Cosine, and Euclidean distances (Yan., et al, 2020).</p>
										    <p>Finding naturally occurring groups in data is the purpose of clustering. For example, in bioinformatics, clustering is used to group genes with similar expression patterns, or in market segmentation, to group clients based on purchase behaviour. By enabling meaningful data segmentation, clustering facilitates meaningful understanding of the data's structure, lowers dimensionality, and enhances decision-making.</p>
										    <p>As mentioned in the lecture cast, there are several different kinds of clustering algorithms, such as Partitioning (K-means Clustering), Density-based, Grid-based, Distribution-based, and Connectivity-based (e.g., Hierarchical Clustering). Every kind has its own advantages and disadvantages and is appropriate for varying types of information and goals (Wong, 2023).</p>
										    <p>K-means clustering is a the most popular method due to its simplicity and efficiency. It aims to partition a dataset into a predefined number of clusters while minimizing the variance within clusters and maximizing the variance between clusters (Li., et al, 2021). Although the K-means clustering approach is widely utilised in several data mining domains, its random seed selection might make it susceptible. To lower this risk, an improved K-means clustering technique called k*-means was proposed together with three optimisation principles.</p>
										    <p>The number of clusters (K) that is selected can have a big impact on how successful K-means clustering is. Plotting the explained variation versus the number of clusters and finding the "elbow" point—where the rate of drop abruptly changes—are techniques like the Elbow Method used to find the ideal K value (Marutho., et al, 2018). Another popular metric for assessing how compact a cluster is the Sum of Squared Error (SSE), which calculates the squared distance between each cluster centroid and each data point.</p>
										    <p>Furthermore, several optimisation strategies and assessment criteria have been put forth to improve K-means' performance and offer a more perceptive assessment of the clusters that are produced. These include using global optimisation methods, optimising the initialization of cluster centroids, and using metrics like separation, cohesion, and the silhouette approach for a more thorough assessment of the clustering outcomes.</p>


								    <h3 style="color: #CFA7F6;"> E-Portfolio activity:</h3>
										<p>The table shows the pathological test results for three individuals:</p>
										
										<table>
										    <tr>
										        <th>Name</th>
										        <th>Gender</th>
										        <th>Fever</th>
										        <th>Cough</th>
										        <th>Test-1</th>
										        <th>Test-2</th>
										        <th>Test-3</th>
										        <th>Test-4</th>
										    </tr>
										    <tr>
										        <td>Jack</td>
										        <td>M</td>
										        <td>Y</td>
										        <td>N</td>
										        <td>P</td>
										        <td>N</td>
										        <td>N</td>
										        <td>A</td>
										    </tr>
										    <tr>
										        <td>Mary</td>
										        <td>F</td>
										        <td>Y</td>
										        <td>N</td>
										        <td>P</td>
										        <td>A</td>
										        <td>P</td>
										        <td>N</td>
										    </tr>
										    <tr>
										        <td>Jim</td>
										        <td>M</td>
										        <td>Y</td>
										        <td>P</td>
										        <td>N</td>
										        <td>N</td>
										        <td>N</td>
										        <td>A</td>
										    </tr>
										</table>
										
										<p>Calculate Jaccard coefficient for the following pairs:</p>
										<ul>
										    <li>(Jack, Mary)</li>
										    <li>(Jack, Jim)</li>
										    <li>(Jim, Mary)</li>
										</ul>
										
										<p>The Jaccard coefficient, also known as the Jaccard similarity index, is defined as:</p>
										<p>J(A,B) = (|A∩B|) / (|A∪B|)</p>
										
										<p>Where:</p>
										<ul>
										    <li>A and B are two sets.</li>
										    <li>|A∩B| is the size of the intersection of the two sets.</li>
										    <li>|A∪B| is the size of the union of the two sets.</li>
										</ul>
										
										<p>To calculate the Jaccard coefficient, treat each individual's attributes as a set, with similar attributes in the intersection set and all attributes in the union set. Handle 'A' as distinct, as it can affect the similarity score. If ignored, only non-'A' attributes are considered. For this calculation, 'A' is treated as a distinct value.</p>
										
										<p><img src="images/ML_Unit5_P1.png" alt="Visual representation of Jaccard Coefficient"></p>
										
										<p>The Jaccard coefficient of 1.0 indicates that Jack and Jim have the same traits, but the attributes of Jim and Mary and Jack and Mary are comparable by about 66.67%.</p>



									
									 <h3 style="color: #CFA7F6;">Reflection:</h3>
										<p>Clustering is a technique with numerous applications in various fields, including market segmentation and bioinformatics. Its nuances have led to an appreciation of its potential and the myriad of applications it holds. The exploration of different metrics and methods has revealed the importance of making informed choices in clustering analysis.</p>
										
										<p>K-means clustering is an attractive choice due to its simplicity and efficiency, but the optimization of the number of clusters and sensitivity to initialization are challenges. The continuous exploration and optimization efforts in clustering algorithms reflect the community's endeavour to refine these methods for better accuracy and insights.</p>
										
										<p>This iterative process of learning, adapting, and optimizing makes the field of clustering a continually evolving landscape.</p>

									
								         <h3 style="color: #CFA7F6;">References:</h3>


									<ol>
									    <li>Yan, X. and Lyu, D. (2020) ‘Comparing dissimilarity metrics for clustering gene into functional modules using machine learning’, <i>Proceedings of the 2020 10th International Conference on Bioscience, Biochemistry and Bioinformatics</i> [Preprint]. doi:10.1145/3386052.3386067.</li>
									    
									    <li>Wong, K.J. (2023) <a href="https://towardsdatascience.com/6-types-of-clustering-methods-an-overview-7522dba026ca">6 types of clustering methods - an overview</a>, Medium.</li>
									    
									    <li>Li, Y. et al. (2021) ‘Customer segmentation using K-means clustering and the adaptive particle swarm optimization algorithm’, <i>Applied Soft Computing</i>, 113, p. 107924. doi:10.1016/j.asoc.2021.107924.</li>
									    
									    <li>Monsalves, B. and Damjan (2022) <a href="https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/#:~:text=The%20various%20types%20of%20clustering,based%20%28Supervised%20Clustering">Types of clustering algorithms in machine learning with examples</a>, Blogs &amp; Updates on Data Science, Business Analytics, AI Machine Learning.</li>
									    
									    <li>Marutho, D. et al. (2018) ‘The determination of cluster number at K-mean using elbow method and purity evaluation on Headline news’, <i>2018 International Seminar on Application for Technology of Information and Communication</i> [Preprint]. doi:10.1109/isemantic.2018.8549751.</li>
									    
									    <li><a href="https://www.geeksforgeeks.org/clustering-in-data-mining/">Clustering in data mining</a> (2022) GeeksforGeeks.</li>
									    
									    <li>Kumari, R. et al. (2016) ‘Anomaly detection in network traffic using K-mean clustering’, <i>2016 3rd International Conference on Recent Advances in Information Technology (RAIT)</i> [Preprint]. doi:10.1109/rait.2016.7507933.</li>
									</ol>



									
								<p></p><a href="Unit 5 - Clustering.ipynb" download>Click to download notebook for Unit 5 - Clustering </a><p>


									
									
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">

							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="js/jquery.min.js"></script>
			<script src="js/jquery.scrolly.min.js"></script>
			<script src="js/jquery.scrollex.min.js"></script>
			<script src="js/browser.min.js"></script>
			<script src="js/breakpoints.min.js"></script>
			<script src="js/util.js"></script>
			<script src="js/main.js"></script>

	</body>
</html>
