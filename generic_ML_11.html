<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	This is an example of a sub page for each module.  It has to be replicated in each module, containing the requested contents -  artefacts, notes, reflections etc
	Ensure you give a different title to each replica and link it to the main module page accordingly.
-->
<html>
	<head>
		<title>Tasweem Beelunkhan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="css/main.css" />
		<noscript><link rel="stylesheet" href="css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><span>Tasweem Beelunkhan</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="About.html">About Me</a></li>
							<li><a href="Launch Module.html">The Data Professional</a></li>
							<li><a href="Module 2.html">Numerical Analysis</a></li>
							<li><a href="Module 4.html">Deciphering Big Data</a></li>
							<li><a href="Module 3.html">Visualising Data</a></li>
							<li><a href="Module 5.html">Machine Learning</a></li>
							<li><a href="Module 6.html">Research Methods and Professional Practice</a></li>
							<li><a href="Project.html">Project</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1> &#10021; Unit 11: Model Selection and Evaluation </h1>
										<h3>Outcomes from the Team Exercises and activites</h3>
									</header>

									
											    <p>Model selection and evaluation are essential steps in machine learning (ML) and deep learning (DL). They ensure that our model performs well not only on our training data but also on new, unseen data. Hastie and et al emphasized this by noting that the best model choice depends on the specific dataset and the problem at hand.</p>
											
											    <p>This unit highlighted that significant challenge in modelling is finding the right balance between bias and variance. A model that's too simple might overlook key patterns, resulting in underfitting. Conversely, a model that's too intricate might excel on training data but falter with new data, a phenomenon known as overfitting. Several techniques, like tweaking the model, expanding data variety, and leveraging multiple models, are employed to address these issues.</p>
											
											    <p>In the journey of choosing the best model, various methods come into play. Some rely on mathematical criteria, like the Bayesian Information Criterion, while others, like cross-validation, are more hands-on. Once a model is developed, its classification efficacy is gauged using metrics like sensitivity and specificity. However, a standout metric that's often highlighted is the Area Under The Curve (AUC) of the Receiver Operating Characteristics (ROC) Curve. It offers a snapshot of the model's ability to differentiate between distinct classes or groups.</p>


									
							        	        <h3 style="color: #CFA7F6;">E-Portfolio Activity: Model Performance Measurement:</h3>

									        <h2>Impact of change of the different parameters on AUC and R2 error</h3>

											    <p>In the python notebook 2 models namely Logistic Regression and One Vs Rest Classifier.</p>
																				
										<h2>1. Logistic Regression with Breast Cancer Dataset</h2>
																				
											    <p>Firstly, we will change the parameter C in the logistic regression model which uses the breast cancer dataset and assess the impact on the AUC and R2.</p>
											
												    <table>
											        <tr>
											            <th>C</th>
											            <th>AUC</th>
											            <th>R2</th>
											        </tr>
											        <tr>
											            <td>0.001</td>
											            <td>0.975054</td>
											            <td>0.768030</td>
											        </tr>
											        <tr>
											            <td>0.010</td>
											            <td>0.984739</td>
											            <td>0.799880</td>
											        </tr>
											        <tr>
											            <td>0.100</td>
											            <td>0.991412</td>
											            <td>0.840486</td>
											        </tr>
											        <tr>
											            <td>1</td>
											            <td>0.994807</td>
											            <td>0.882230</td>
											        </tr>
											        <tr>
											            <td>10</td>
											            <td>0.995468</td>
											            <td>0.903958</td>
											        </tr>
											        <tr>
											            <td>100</td>
											            <td>0.995204</td>
											            <td>0.903635</td>
											        </tr>
											    </table>
											
																				
											    <p>Both the AUC score and the R2 tends to increase as the regularization strength decreases (i.e., as C increases), but it starts to plateau and slightly decrease beyond C=10.</p>
											
										 <h2>2. One Vs Rest Classifier with Iris Dataset</h2>
											    <p>Now, we will assess the impact of changing the parameter C on the One Vs Rest Classifier which uses the iris data set and assess the impact on the AUC and R2.</p>
											    <p>When using the OneVsRestClassifier (OvR) approach with a classifier like SVC, the strategy is to fit one classifier per class, where each classifier treats one of the classes as the positive class and all the other classes combined as the negative class.
											    In the analysis, I computed the Macro-average AUC (often referred to as "Average AUC") as it gives a quick, equal-weighted summary metric of the model's performance across all classes.</p>
																		
												    <table>
											        <tr>
											            <th>C</th>
											            <th>Average AUC</th>
											            <th>R2</th>
											        </tr>
											        <tr>
											            <td>0.001</td>
											            <td>0.8948</td>
											            <td>-6.3935</td>
											        </tr>
											        <tr>
											            <td>0.010</td>
											            <td>0.8943</td>
											            <td>-4.3611</td>
											        </tr>
											        <tr>
											            <td>0.100</td>
											            <td>0.8994</td>
											            <td>-14.0893</td>
											        </tr>
											        <tr>
											            <td>1.000</td>
											            <td>0.9055</td>
											            <td>-30.3932</td>
											        </tr>
											        <tr>
											            <td>10.000</td>
											            <td>0.9102</td>
											            <td>-78.7363</td>
											        </tr>
											        <tr>
											            <td>100.000</td>
											            <td>0.9105</td>
											            <td>-393.6397</td>
											        </tr>
											    </table>
											
											
											    <p>• Average AUC:</p>
											    <p>As the C parameter increases, the Average AUC slightly increases, suggesting that the classifier's discrimination ability improves with higher regularization strengths. However, the gains plateau around C=10 and C=100.</p>
											
											    <p>• R2:</p>
											    <p>The negative R2 readings are rare for this measure. R2 is typically between 0 and 1, with higher values suggesting a better match. Negative numbers indicate that the model fits the data less well than a straight line.
											    As C increases, the R2 values become progressively negative, suggesting that the model's predictions diverge from the actual labels. In the context of the OneVsRestClassifier for multi-class classification, these negative R2 values imply that the decision function scores may be misaligned with the genuine multi-class labels. For this sort of model, the metric may not be as informative as it is for ordinary regression models.</p>
											
																				

									        <h3 style="color: #CFA7F6;">Reflection & Practical Use:</h3>

											    <p>The highlights the importance of model selection and evaluation in data science, emphasizing the need for models to provide meaningful insights on new data and generalize their performance based on the dataset and problem definition.</p>
											
											    <p>Classification metrics, particularly the AUC of the ROC Curve, are crucial for gauging a model's capability to distinguish between classes. AUC is often used in various fields, such as medical diagnosis and finance. The unit also highlighted the impact of parameter changes on AUC and R<sup>2</sup>, emphasizing the importance of hyperparameter tuning in machine learning. The insights gained from the unit, such as the plateauing of AUC beyond a certain C value and negative R<sup>2</sup> values in multi-class scenarios, underscore the need for iterative model refinement and questioning every metric.</p>
											
											    <p>The lessons learned from this unit will serve as a guiding light for data scientists, reminding them of the importance of model selection and evaluation in translating data into real-world impact.</p>
																			

									       <h3 style="color: #CFA7F6;">References:</h3>
									
									
											<ul>
											    <li>Hastie, T., Tibshirani, R. and Friedman, J., 2009. <em>The Elements of Statistical Learning</em>. Springer.</li>
											    <li>Geman, S., Bienenstock, E. and Doursat, R., 1992. Neural networks and the bias/variance dilemma. <em>Neural Computation</em>, 4(1), pp.1-58.</li>
											    <li>Goodfellow, I., Bengio, Y. and Courville, A., 2016. <em>Deep Learning</em>. MIT Press.</li>
											    <li>Burnham, K.P. and Anderson, D.R., 2002. Model selection and multimodel inference: a practical information-theoretic approach. Springer.</li>
											    <li>Kohavi, R., 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. <em>Proceedings of the 14th International Joint Conference on Artificial Intelligence</em>, 2, pp.1137-1143.</li>
											    <li>Fawcett, T., 2006. An introduction to ROC analysis. <em>Pattern Recognition Letters</em>, 27(8), pp.861-874.</li>
											    <li>Bradley, A.P., 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. <em>Pattern Recognition</em>, 30(7), pp.1145-1159</li>
											</ul>

									

									<p></p><a href="Unit 11- Model Performance Measurement.ipynb" download>Click to download notebook for Unit 11- Model Performance Measurement </a><p>
									
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">

							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="js/jquery.min.js"></script>
			<script src="js/jquery.scrolly.min.js"></script>
			<script src="js/jquery.scrollex.min.js"></script>
			<script src="js/browser.min.js"></script>
			<script src="js/breakpoints.min.js"></script>
			<script src="js/util.js"></script>
			<script src="js/main.js"></script>

	</body>
</html>
