<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	This is an example of a sub page for each module.  It has to be replicated in each module, containing the requested contents -  artefacts, notes, reflections etc
	Ensure you give a different title to each replica and link it to the main module page accordingly.
-->
<html>
	<head>
		<title>Tasweem Beelunkhan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="css/main.css" />
		<noscript><link rel="stylesheet" href="css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><span>Tasweem Beelunkhan</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="About.html">About Me</a></li>
							<li><a href="Launch Module.html">The Data Professional</a></li>
							<li><a href="Module 2.html">Numerical Analysis</a></li>
							<li><a href="Module 4.html">Deciphering Big Data</a></li>
							<li><a href="Module 3.html">Visualising Data</a></li>
							<li><a href="Module 5.html">Machine Learning</a></li>
							<li><a href="Module 6.html">Research Methods and Professional Practice</a></li>
							<li><a href="Project.html">Project</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1> &#10021; Unit 6: Clustering with Python </h1>
										<h3>Outcomes from the Team Exercises and activites</h3>
									</header>


									<p>K-Means is an iterative clustering algorithm that aims to partition a dataset into K distinct, non-overlapping subsets (or clusters). The goal is to minimize the within-cluster variances and maximize the between-cluster variances.</p>
									
									<p>In other words, the K-means method is designed to partition two-way, two-mode data (that is, N objects each having measurements on P variables) into K classes (C1, C2, ::: ,CK), where Ck is the set of nk objects in cluster k, and K is given (<cite>Steinley, 2006</cite>).</p>
									
									<h3>Algorithm Process (MacQueen 1967)</h3>
									<ol>
									    <li><strong>Initialization:</strong> Select K cluster centres (centroids) from the outset. You can use a heuristic or choose to do this at random.</li>
									    <li><strong>Assigning:</strong> Put every data point in relation to the closest centroid. Thus, K clusters are formed.</li>
									    <li><strong>Update:</strong> Determine each cluster's average point position by taking the mean of all the points and moving the centroid there.</li>
									    <li><strong>Convergence:</strong> The algorithm has reached its point of convergence when steps 2 and 3 are repeated until the centroids no longer fluctuate noticeably.</li>
									</ol>
									
									<p>Python's Scikit-Learn library provides an easy-to-use implementation of the K-Means algorithm.</p>
									
									<h3>Important Considerations (Lloyd, 1982)</h3>
									<ul>
									    <li>Scaling features is critical so that one trait does not dominate the grouping owing to its higher scale.</li>
									    <li>Choosing K: Use methods such as the Elbow method, which plots variance as a function of cluster number. The curve's "elbow" reflects an ideal value for K (a trade-off between accuracy and computing expense).</li>
									    <li>Multiple Initializations: Run the algorithm with different initializations several times and select the best result.</li>
									</ul>

							<h3 style="color: #CFA7F6;">ACTIVITY: TASK A</h3>	
									<p>Perform K-Means clustering on the dataset, iris.csv (from the UCI Machine Learning Repository). Before using the data for clustering, you might have to remove a few columns because the K-Means algorithm involves the calculation of Euclidian distance. You can choose various values of K; however, you must also choose K = 3 in this case. Upon clustering at K = 3, check how much similar your three clusters are as compared to the labels of species – setosa, versicolour, and verginica.</p>
									
									<h3>1. Inspection of Dataset:</h3>
									<p>Firstly, the dataset is loaded, and its first few rows are inspected.</p>
									<img src="images/ML_Unit6_P1.png" alt="PIC 1">
									<p>The dataset contains the following columns:</p>
									<ul>
									    <li>sepal_length</li>
									    <li>sepal_width</li>
									    <li>petal_length</li>
									    <li>petal_width</li>
									    <li>species</li>
									</ul>
									
									<h3>2. Data Preparation:</h3>
									<p>The first four columns are numeric and suitable for clustering. The last column, species, contains the labels that we can use to compare the clustering results. Thus, we drop the last column.</p>
									
									<h3>3. K-Means Clustering:</h3>
									<p>We'll perform K-Means clustering for a range of K values and compute the silhouette score for each value.</p>
									<p>The silhouette scores for various values of K are as follows:</p>
									<table>
									    <tr>
									        <th>K Value</th>
									        <th>Silhouette Score</th>
									    </tr>
									    <tr><td>1</td><td>0.6808</td></tr>
									    <tr><td>2</td><td>0.5526</td></tr>
									    <tr><td>3</td><td>0.4978</td></tr>
									    <tr><td>4</td><td>0.4885</td></tr>
									    <tr><td>5</td><td>0.3682</td></tr>
									    <tr><td>6</td><td>0.3577</td></tr>
									    <tr><td>7</td><td>0.3591</td></tr>
									    <tr><td>8</td><td>0.3424</td></tr>
									    <tr><td>9</td><td>0.3166</td></tr>
									</table>
									
									<h3>4. Silhouette Score Visualization:</h3>
									<p>Then, we'll visualize the silhouette scores to determine the optimal number of clusters.</p>
									<img src="images/ML_Unit6_P3.png" alt="PIC 3">
									<p>K=2 has the greatest silhouette score, as can be seen from the plot of silhouette scores for different values of K. K=3, on the other hand, is still a logical option and is not far behind, particularly considering the iris dataset's three unique species.</p>
									<p>In conclusion, K=3 is better in line with the dataset's natural structure and our past understanding of the iris species, even if K=2 offers the greatest silhouette score. This highlights how crucial it is to comprehend the data and have subject expertise when making judgements based on clustering results.</p>
									
									<h3>5. Clustering with K=3:</h3>
									<p>Thus, the K-Means clustering is used on the dataset for K=3.</p>
									
									<h3>6. Cross-tabulation:</h3>
									<p>Then, a cross-tabulation or confusion matrix between the predicted clusters and the actual species labels is created. This is to compare the clusters to the given species labels to see how closely the clustering matches the actual species categories.</p>
									<img src="images/ML_Unit6_P4.png" alt="PIC 4">
									<p>The following findings are obtained from the cross-tabulation of the actual species and the anticipated clusters:</p>
									<ul>
									    <li>It is accurate to classify all 50 of the setosa species samples into cluster 1.</li>
									    <li>Two samples are sorted into cluster 2 and 48 samples are placed into cluster 0 for the versicolor species.</li>
									    <li>Thirteen samples are placed in cluster 0 for the virginica species, while thirty-six samples are placed in cluster 2.</li>
									    <li>For the setosa species, the grouping appears to function well overall, but there is considerable overlap with versicolor and virginica.</li>
									</ul>
									
									
							  <h3 style="color: #CFA7F6;">Reflection:</h3>									
								       <p>K-Means clustering is more than a simple algorithm. Its real-world applications are extensive and significant, encompassing a wide range of sectors. Mastering K-Means means having a flexible tool at their disposal that can extract insights from data, improve machine learning models, and solve practical business issues. As businesses and organisations continue to rely on data-driven decision-making, K-Means and its applications in real life will become increasingly important.</p>

									
							  <h3 style="color: #CFA7F6;">References:</h3>
									<ol>
									    <li>
									        Steinley, Douglas. (2006) ‘K‐means clustering: A half‐century synthesis’, <i>British Journal of Mathematical and Statistical Psychology</i>, 59(1), pp. 1–34. <a href="https://doi.org/10.1348/000711005x48266" target="_blank">doi:10.1348/000711005x48266</a>.
									    </li>
									    <li>
									        MacQueen, J.B. (1967) Some Methods for Classification and Analysis of Multivariate Observations. In: <i>Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics</i>, University of California Press, Berkeley, 281-297.
									    </li>
									    <li>
									        Lloyd, S. (1982) ‘Least squares quantization in PCM’, <i>IEEE Transactions on Information Theory</i>, 28(2), pp. 129–137. <a href="https://doi.org/10.1109/tit.1982.1056489" target="_blank">doi:10.1109/tit.1982.1056489</a>.
									    </li>
									    <li>
									        Pedregosa, F. et al. (1970) Scikit-Learn: Machine learning in Python, <i>Journal of Machine Learning Research</i>. Available at: <a href="https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" target="_blank">https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</a> (Accessed: 19 October 2023). 
									    </li>
									</ol>
									

							<p></p><a href="Unit 6 - Clustering with Python.ipynb" download>Click to download notebook for Unit 6 - Clustering with Python </a><p>  
									
									
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">

							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="js/jquery.min.js"></script>
			<script src="js/jquery.scrolly.min.js"></script>
			<script src="js/jquery.scrollex.min.js"></script>
			<script src="js/browser.min.js"></script>
			<script src="js/breakpoints.min.js"></script>
			<script src="js/util.js"></script>
			<script src="js/main.js"></script>

	</body>
</html>
