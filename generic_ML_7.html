<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	This is an example of a sub page for each module.  It has to be replicated in each module, containing the requested contents -  artefacts, notes, reflections etc
	Ensure you give a different title to each replica and link it to the main module page accordingly.
-->
<html>
	<head>
		<title>Tasweem Beelunkhan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="css/main.css" />
		<noscript><link rel="stylesheet" href="css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><span>Tasweem Beelunkhan</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="About.html">About Me</a></li>
							<li><a href="Launch Module.html">The Data Professional</a></li>
							<li><a href="Module 2.html">Numerical Analysis</a></li>
							<li><a href="Module 4.html">Deciphering Big Data</a></li>
							<li><a href="Module 3.html">Visualising Data</a></li>
							<li><a href="Module 5.html">Machine Learning</a></li>
							<li><a href="Module 6.html">Research Methods and Professional Practice</a></li>
							<li><a href="Project.html">Project</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1> &#10021; Unit 7: Introduction to Artificial Neural Networks (ANNs) </h1>
										<h3>Outcomes from the Team Exercises and activites</h3>
									</header>

           
											<p>Machine learning is the process by which a machine (i.e., computer) determines how input data is processed and predicts outcomes when presented with fresh data. A machine learning method based on the notion of a human neuron is known as an artificial neural network (<cite>Han., et al, 2018</cite>).</p>
											
											<h3>Single Layer Perceptron (SLP)</h3>
											<p>In this unit, I learned about the Single Layer Perceptron (SLP). It is a basic form of a perceptron, consisting of one layer connecting inputs and output. It is a feed-forward network where data flows in a single direction without looping back. The perceptron uses weights to determine the importance of each input feature and an adder function to combine the inputs and their respective weights. A bias is added to form the induced local field.</p>
											
											<h3>Multilayer Perceptron (MLP)</h3>
											<p>The Multilayer Perceptron (MLP) is an evolution of the SLP and includes one or more hidden layers between its input and output layers. The number of neurons in the input layer matches the dataset's attributes, while the output layer corresponds to the dataset's class labels.</p>
											
											<h3>Initialization & Activation Functions</h3>
											<p>Proper initialization of weights and biases is crucial for a model's performance. The best approach often depends on the chosen activation function, with specific initialization intervals suggested for functions like tanh and sigmoid. Various activation functions exist, such as the threshold function, which determines a neuron's output based on its induced local field.</p>




									        <h3 style="color: #CFA7F6;"> E-Portfolio activity:Perceptron Examples:</h3>

											<h3>EXAMPLE 1: SIMPLE PERCEPTRON</h3>
											<p>Dr. Mike Lakoju's notebook discusses the implementation of a simple perceptron using the Numpy library, emphasizing the need for optimization when dealing with extensive datasets in neural networks. The notebook begins by defining inputs as Numpy arrays with two values: 45 and 25, which are foundational data points for the perceptron. The type and structure of these inputs are verified, and their first value is confirmed as 45. The weights associated with these inputs are defined as Numpy arrays with values 0.7 and 0.1, which are validated as 0.7.</p>
											<p>The notebook also introduces a sum function to compute the weighted sum of the inputs, which serves as the raw output of the perceptron before any activation function is applied. The dot product is highlighted as a computationally efficient method for calculating the weighted sum, which is crucial when dealing with large datasets.</p>
											<p>The notebook sets the groundwork for constructing a perceptron by defining its inputs and weights and hinting at the methods for processing these using Numpy's powerful array operations.</p>
											
											<h3>EXAMPLE 2: PERCEPTRON & OPERATORS</h3>
											<p>The code defines the inputs as a matrix containing all possible binary input pairs for the AND operator, with each row representing a distinct input pair. The expected outputs are provided as a vector, mirroring the AND operator's behavior. Two weights are initialized for the perceptron, one for each input, set to 0.0, indicating no bias towards any input. A learning rate of 0.1 is set, which influences the adjustment magnitude of weights during the learning process.</p>
											<p>An activation function, step_function, is defined, which returns a binary output based on the input's sum of weighted inputs. This step function acts as the threshold mechanism for the perceptron, deciding its final output. A method, cal_output, is introduced to calculate the perceptron's output for a given input instance.</p>
											<p>The notebook meticulously defines the necessary inputs, outputs, and parameters, and introduces mechanisms to compute the perceptron's output. This groundwork lays the groundwork for training and evaluating the perceptron's accuracy in emulating the AND operator.</p>
											
											<h3>EXAMPLE 3: MULTI-LAYER PERCEPTRON</h3>
											<p>The notebook "Unit07 Ex3 multi-layer Perceptron" introduces the sigmoid function, a widely used activation function in neural networks due to its S-shaped curve. It compresses any input into a value between 0 and 1, with practical examples demonstrating its importance. The inputs for this perceptron are defined as a matrix with all possible binary combinations, suggesting that the perceptron might be trained on logical operators or binary patterns. The shape and content of these inputs are verified to ensure their correctness.</p>
											<p>The notebook lays the groundwork for a multi-layer perceptron by setting up the necessary functions, parameters, and data structures. It then outlines the architecture of the multi-layer perceptron, suggesting a layered architecture fundamental to deep learning models. The inputs are structured as a matrix embracing all binary combinations, mirroring patterns seen in logical operations. This choice of inputs indicates that the perceptron could be oriented towards understanding binary patterns or logical operators.</p>
											<p>The notebook serves as a primer, meticulously laying the groundwork for a multi-layer perceptron by establishing the sigmoid function, hinting at the perceptron's architecture, and setting up the inputs. This sets the stage for deeper dives into forward propagation, backpropagation, and the nuances of neural network training.</p>

									        <h3 style="color: #CFA7F6;">Reflection & Practical Use:</h3>
									
								                	<p>Artificial Neural Networks (ANNs) have emerged as a paradigm-shifting methodology in data science, offering nuanced insights and predictions across vast datasets. This unit delves into the applicability of ANNs, from the rudimentary Single Layer Perceptron (SLP) to the more sophisticated Multilayer Perceptron (MLP), elucidating their relevance in the empirical world of data science.</p>

											<p>The SLP, analogized to linear regression in traditional statistics, offers a foundational perspective on neural computation, providing an efficient and interpretable model for practical data science tasks.</p>
											
											<p>The MLP's layered architecture is primed to extract latent features and nonlinear patterns, making it suitable for various datasets, such as genomic data exploration, time-series forecasting in finance, and deciphering intricate consumer behavior in digital marketing.</p>
											
											<p>Dr. Mike Lakoju's work offers a hands-on exploration of ANNs using Python's Numpy library, highlighting the symbiotic relationship between computational efficiency and large-scale data analytics. The practical implementation of the perceptron, optimized using Numpy's array operations, highlights the need for data scientists to understand theoretical underpinnings and be adept at leveraging computational tools for scalable analytics.</p>
											
											<p>ANNs, particularly MLPs, are making inroads into diverse sectors, such as healthcare, finance, and e-commerce. Predictive analytics using ANNs in medical imaging aids radiologists in early detection of anomalies, while time-series forecasting aids in stock market predictions, risk assessment, and fraud detection. Furthermore, ANNs provide a resilient framework for continuous learning and adaptation as data scientists grapple with challenges like data drift, concept drift, and the curse of dimensionality.</p>
											
											<p>In conclusion, the convergence of ANNs and data science marks a transformative phase in analytics, guiding data scientists towards meaningful insights and actionable results.</p>
	
									
									       <h3 style="color: #CFA7F6;">References:</h3>
											<ol>
											    <li>Han, S.-H. et al. (2018) ‘Artificial Neural Network: Understanding the basic concepts without mathematics’, Dementia and Neurocognitive Disorders, 17(3), p. 83. <a href="https://doi.org/10.12779/dnd.2018.17.3.83" target="_blank">doi:10.12779/dnd.2018.17.3.83</a>.</li>
											    <li>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep Learning. MIT Press.</li>
											    <li>LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. Nature, 521(7553), pp.436-444.</li>
											    <li>Rumelhart, D.E., Hinton, G.E. and Williams, R.J., 1986. Learning representations by back-propagating errors. Nature, 323(6088), pp.533-536.</li>
											    <li>Bishop, C.M., 1995. Neural Networks for Pattern Recognition. Oxford University Press.</li>
											    <li>Schmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural Networks, 61, pp.85-117.</li>
											    <li>Cybenko, G., 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), pp.303-314.</li>
											    <li>He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.770-778.</li>
											    <li>Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), pp.1929-1958.</li>
											    <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., ... and Polosukhin, I., 2017. Attention is all ...</li>
											</ol>
		

									
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">

							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="js/jquery.min.js"></script>
			<script src="js/jquery.scrolly.min.js"></script>
			<script src="js/jquery.scrollex.min.js"></script>
			<script src="js/browser.min.js"></script>
			<script src="js/breakpoints.min.js"></script>
			<script src="js/util.js"></script>
			<script src="js/main.js"></script>

	</body>
</html>
